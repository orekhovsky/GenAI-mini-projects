{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orekhovsky/GenAI-mini-projects/blob/main/simple_RAG_with_Gigachat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Без векторной бд"
      ],
      "metadata": {
        "id": "KYhuJURewcYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-bioasq/data/passages.parquet/part.0.parquet\")\n",
        "df_test = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-bioasq/data/test.parquet/part.0.parquet\")\n",
        "\n",
        "import ast\n",
        "\n",
        "# преобразование столбца relevant_passage_ids из строки в список\n",
        "df_test['relevant_passage_ids'] = df_test['relevant_passage_ids'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "xCgZ-YjFwidm",
        "outputId": "673a87e1-76ef-4b78-bf77-028918df37bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gigachat, transformers"
      ],
      "metadata": {
        "id": "5s4wCOof18Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community"
      ],
      "metadata": {
        "id": "boONLerW1ugn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция для формирования контекста по вопросу из relevant_passage_ids\n",
        "def perform_rag(question, df, df_test):\n",
        "\n",
        "    matching_rows = df_test[df_test['question'] == question]\n",
        "\n",
        "    # Получение индекса 'id' вместо стандартного числового индекса\n",
        "    question_index = matching_rows.index[0]  # Здесь index вернёт значение из индекса 'id'\n",
        "    print(f\"Question ID: {question_index}\")\n",
        "\n",
        "    # Получение релевантных чанков из корпуса текстов\n",
        "    relevant_passage_ids = df_test.loc[question_index, 'relevant_passage_ids']\n",
        "    relevant_passages = [df.loc[i, 'passage'] for i in relevant_passage_ids]\n",
        "    print(relevant_passage_ids)\n",
        "    # Формирование входных данных для модели\n",
        "    context = \" \".join(relevant_passages)\n",
        "\n",
        "    return context"
      ],
      "metadata": {
        "id": "VCWssppFA1Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain.chat_models.gigachat import GigaChat\n",
        "\n",
        "\n",
        "auth = userdata.get('SBER_AUTH')\n",
        "\n",
        "\n",
        "llm = GigaChat(\n",
        "    credentials=auth,\n",
        "    model='GigaChat:latest',\n",
        "    verify_ssl_certs=False,\n",
        "    profanity_check=False\n",
        ")\n",
        "\n",
        "\n",
        "q1 = 'Is Hirschsprung disease a mendelian or a multifactorial disorder?'\n",
        "context = perform_rag(q1, df, df_test)\n",
        "\n",
        "\n",
        "input_text = f\"\"\"Answer the user's question.\n",
        "Use only the information from the context. If the context does not contain enough information to answer the question, let the user know.\n",
        "Context: {context}\n",
        "Question: {q1}\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "response = llm.predict(input_text)\n",
        "\n",
        "\n",
        "print(f\"Question: {q1}\")\n",
        "print(f\"Answer: {response}\")\n"
      ],
      "metadata": {
        "id": "IIuL0Cnpwud_",
        "outputId": "2822e527-802d-4cec-d4f8-f65e315d19a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question ID: 0\n",
            "[20598273, 6650562, 15829955, 15617541, 23001136, 8896569, 21995290, 12239580, 15858239]\n",
            "Question: Is Hirschsprung disease a mendelian or a multifactorial disorder?\n",
            "Answer: Hirschsprung disease (HSCR) is considered to be a **multifactorial** disorder rather than a Mendelian disorder. This means that the disease results from the combined effects of multiple genetic factors and possibly environmental influences. While there are specific genes like RET and EDNRB that play key roles in the development of HSCR, the disease often exhibits variable expressivity and incomplete penetrance, which complicates its inheritance pattern. Additionally, many cases do not follow a clear Mendelian inheritance pattern, further supporting the multifactorial nature of the disease.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['answer'][0]"
      ],
      "metadata": {
        "id": "TUNpqYya5B4n",
        "outputId": "ac978bc3-4935-4c63-da97-e2ff8ec1cb05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Coding sequence mutations in RET, GDNF, EDNRB, EDN3, and SOX10 are involved in the development of Hirschsprung disease. The majority of these genes was shown to be related to Mendelian syndromic forms of Hirschsprung's disease, whereas the non-Mendelian inheritance of sporadic non-syndromic Hirschsprung disease proved to be complex; involvement of multiple loci was demonstrated in a multiplicative model.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score"
      ],
      "metadata": {
        "id": "16gzeS1P5n7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "true_answer = df_test['answer'][0]\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(true_answer, response)\n",
        "\n",
        "# метрики\n",
        "print(\"ROUGE-1:\", scores['rouge1'])\n",
        "print(\"ROUGE-2:\", scores['rouge2'])\n",
        "print(\"ROUGE-L:\", scores['rougeL'])\n"
      ],
      "metadata": {
        "id": "9_jsyRpK9Ki1",
        "outputId": "229c1d8b-af2d-489f-f1e9-aaca42f40d8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1: Score(precision=0.2857142857142857, recall=0.39344262295081966, fmeasure=0.3310344827586207)\n",
            "ROUGE-2: Score(precision=0.08433734939759036, recall=0.11666666666666667, fmeasure=0.0979020979020979)\n",
            "ROUGE-L: Score(precision=0.14285714285714285, recall=0.19672131147540983, fmeasure=0.16551724137931034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Эксперименты с другими моделями"
      ],
      "metadata": {
        "id": "cMEuyLG-rzzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "messages = [\n",
        "\t{\n",
        "\t\t\"role\": \"user\",\n",
        "\t\t\"content\": \"What is the capital of France?\"\n",
        "\t}\n",
        "]\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "\tmessages=messages,\n",
        "\tmax_tokens=500\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "id": "mJfMU3fir6td",
        "outputId": "89b17e71-b8f3-4973-a938-208afaf5919e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "(Request ID: wyl1DdCWrhvz91Qnqktcp)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.3-70B-Instruct/v1/chat/completions",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a9c00a845bb5>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"meta-llama/Llama-3.3-70B-Instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    890\u001b[0m         )\n\u001b[1;32m    891\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0;34mf\"\\n\\nBad request for {endpoint_name} endpoint:\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mendpoint_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\\n\\nBad request:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             )\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: (Request ID: wyl1DdCWrhvz91Qnqktcp)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
          ]
        }
      ]
    }
  ]
}